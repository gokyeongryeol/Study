# Study materials in notes, seminars, and surveys

## Notes

Notes are self-written by referring the following classes, online course, papers: 
 - AI501, Machine learning for AI, KAIST
 - AI505, Optimization for AI, KAIST
 - CS285, Deep Reinforcement Learning, UC Berkeley
    - Online course link : http://rail.eecs.berkeley.edu/deeprlcourse/
 - Stochastic Process
    - Rasmussen, Carl Edward. "Gaussian processes in machine learning." Summer school on machine learning. Springer, Berlin, Heidelberg, 2003.
    - Quinonero-Candela, Joaquin, and Carl Edward Rasmussen. "A unifying view of sparse approximate Gaussian process regression." The Journal of Machine Learning Research 6 (2005): 1939-1959.
    - Titsias, Michalis. "Variational learning of inducing variables in sparse Gaussian processes." Artificial intelligence and statistics. PMLR, 2009.
    - Gershman, Samuel J., and David M. Blei. "A tutorial on Bayesian nonparametric models." Journal of Mathematical Psychology 56.1 (2012): 1-12.
    - Teh, Yee Whye, et al. "Hierarchical dirichlet processes." Journal of the american statistical association 101.476 (2006): 1566-1581.
 - Diffusion Model
    - Song, Yang, and Stefano Ermon. "Generative modeling by estimating gradients of the data distribution." Advances in Neural Information Processing Systems 32 (2019).
    - Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." Advances in Neural Information Processing Systems 33 (2020): 6840-6851.
    - Song, Jiaming, Chenlin Meng, and Stefano Ermon. "Denoising diffusion implicit models." arXiv preprint arXiv:2010.02502 (2020).
    - Nichol, Alexander Quinn, and Prafulla Dhariwal. "Improved denoising diffusion probabilistic models." International Conference on Machine Learning. PMLR, 2021.
    - Dhariwal, Prafulla, and Alexander Nichol. "Diffusion models beat gans on image synthesis." Advances in Neural Information Processing Systems 34 (2021): 8780-8794.

## Seminar

Presentation files for regular lab seminar about recent deep learning papers (mostly) accepted in top-tier conference of aritificial intelligence (e.g. ICML, NeurIPs, ICLR).

 - Variational Inference
   - [Gaussian process prior variational autoencoders](./seminar/Gaussian_Process_Prior_Variational_Autoencoder.pdf)
   - [Study on Latent Representation and Clustering](./seminar/Study_on_Latent_Representation_and_Clustering.pdf)
   - [Stochastic Neural Networks with Variational Inference](./seminar/Stochastic_Neural_Networks_with_Variational_Inference.pdf)
   - [Variational Interaction Information Maximization for Cross-domain Disentanglement](./seminar/Variational_Interaction_Information_Maximization_for_Cross-domain_Disentanglement.pdf)
 - Meta-learning
   - [Neural Process Family](./seminar/Neural_Process_Family.pdf)
   - [The Functional Neural Process](./seminar/The_Functional_Neural_Process.pdf)
   - [Automated Relational Meta Learning](./seminar/Automated_Relational_Meta_Learning.pdf)
   - [NeurIPS 2020 paper review](./seminar/NeurIPS_2020_paper_review.pdf)
- Reinforcement learning
   - [Never Give Up - Learning Directed Exploration Strategies](./seminar/Never_Give_Up-Learning_Directed_Exploration_Strategies.pdf)
   - [Deep Reinforcement Learning Amidst Lifelong Non-stationarity](./seminar/Deep_Reinforcement_Learning_Amidst_Lifelong_Non-stationarity.pdf)
   - [Soft Q-learning with Mutual Information Regularization](./seminar/Soft_Q-learning_with_Mutual_Information_Regularization.pdf)
- Optimizer
   - [Sharpness-Aware Minimization for Efficiently Improving Generalization](./seminar/Sharpness-Aware_Minimization_for_Efficiently_Improving_Generalization.pdf)

## Survey

Survey on basic (and advanced) deep learning modules which many follow-up studies regard as baselines.
In macroscopic view, these can be categorized into

 - Regularization
 - Convolutional Neural Network
 - Recurrent Neural Network
 - Autoencoder
 - ...
