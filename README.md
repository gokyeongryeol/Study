# Study materials in notes, seminars, and surveys

## Notes

Notes are self-written by referring to the following classes, online courses, and papers: 
 - AI501, Machine learning for AI, KAIST
 - AI505, Optimization for AI, KAIST
 - CS285, Deep Reinforcement Learning, UC Berkeley
    - Online course link: http://rail.eecs.berkeley.edu/deeprlcourse/
 - Stochastic Process
    - Rasmussen, Carl Edward. "Gaussian processes in machine learning." Summer school on machine learning. Springer, Berlin, Heidelberg, 2003.
    - Quinonero-Candela, Joaquin, and Carl Edward Rasmussen. "A unifying view of sparse approximate Gaussian process regression." The Journal of Machine Learning Research 6 (2005): 1939-1959.
    - Titsias, Michalis. "Variational learning of inducing variables in sparse Gaussian processes." Artificial intelligence and statistics. PMLR, 2009.
    - Gershman, Samuel J., and David M. Blei. "A tutorial on Bayesian nonparametric models." Journal of Mathematical Psychology 56.1 (2012): 1-12.
    - Teh, Yee Whye, et al. "Hierarchical Dirichlet processes." Journal of the American Statistical Association 101.476 (2006): 1566-1581.
 - Diffusion Model
    - Song, Yang, and Stefano Ermon. "Generative modeling by estimating gradients of the data distribution." Advances in Neural Information Processing Systems 32 (2019).
    - Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." Advances in Neural Information Processing Systems 33 (2020): 6840-6851.
    - Song, Jiaming, Chenlin Meng, and Stefano Ermon. "Denoising diffusion implicit models." arXiv preprint arXiv:2010.02502 (2020).
    - Nichol, Alexander Quinn, and Prafulla Dhariwal. "Improved denoising diffusion probabilistic models." International Conference on Machine Learning. PMLR, 2021.
    - Dhariwal, Prafulla, and Alexander Nichol. "Diffusion models beat gans on image synthesis." Advances in Neural Information Processing Systems 34 (2021): 8780-8794.

## Seminar

Presentation files for regular lab seminars about recent deep learning papers (mostly) accepted in top-tier conferences (e.g. ICML, NeurIPs, ICLR).

 - Data-centric AI
   - [Ensemble in deep neural network and Uncertainty quantification](./seminar/Ensemble_in_deep_neural_network_and_Uncertainty_quantification.pdf)
   - [Deep Classifiers with Label Noise Modeling and Distance Awareness](./seminar/Deep_Classifiers_with_Label_Noise_Modeling_and_Distance_Awareness.pdf)
   - [Feature Space Particle Inference for Neural Network Ensembles](./seminar/Feature_Space_Particle_Inference_for_Neural_Network_Ensembles.pdf)
   - [Adaptive Data Subset Selection](./seminar/Adaptive_Data_Subset_Selection.pdf)
   - [Data Shapley, Equitable Valuation of Data for Machine Learning](./seminar/Data_Shapley_Equitable_Valuation_of_Data_for_Machine_Learning.pdf)
   - [Mitigating noisy labels and dataset bias in machine learning](./seminar/Mitigating_noisy_labels_and_dataset_bias_in_machine_learning.pdf)
 - Variational Inference
   - [Gaussian process prior variational autoencoders](./seminar/Gaussian_Process_Prior_Variational_Autoencoder.pdf)
   - [Study on Latent Representation and Clustering](./seminar/Study_on_Latent_Representation_and_Clustering.pdf)
   - [Stochastic Neural Networks with Variational Inference](./seminar/Stochastic_Neural_Networks_with_Variational_Inference.pdf)
   - [Variational Interaction Information Maximization for Cross-domain Disentanglement](./seminar/Variational_Interaction_Information_Maximization_for_Cross-domain_Disentanglement.pdf)
 - Meta-learning
   - [Neural Process Family](./seminar/Neural_Process_Family.pdf)
   - [The Functional Neural Process](./seminar/The_Functional_Neural_Process.pdf)
   - [Automated Relational Meta Learning](./seminar/Automated_Relational_Meta_Learning.pdf)
   - [NeurIPS 2020 paper review](./seminar/NeurIPS_2020_paper_review.pdf)
- Reinforcement learning
   - [Never Give Up - Learning Directed Exploration Strategies](./seminar/Never_Give_Up-Learning_Directed_Exploration_Strategies.pdf)
   - [Deep Reinforcement Learning Amidst Lifelong Non-stationarity](./seminar/Deep_Reinforcement_Learning_Amidst_Lifelong_Non-stationarity.pdf)
   - [Soft Q-learning with Mutual Information Regularization](./seminar/Soft_Q-learning_with_Mutual_Information_Regularization.pdf)
- Optimizer
   - [Sharpness-Aware Minimization for Efficiently Improving Generalization](./seminar/Sharpness-Aware_Minimization_for_Efficiently_Improving_Generalization.pdf)

## Survey

Survey on basic (and advanced) deep learning modules which many follow-up studies regard as baselines.
In the macroscopic view, these can be categorized into

 - Regularization
 - Convolutional Neural Network
 - Recurrent Neural Network
 - Autoencoder
 - ...
