# Study materials in notes, seminars, and surveys

## Notes

Notes are self-written by referring the following classes, online course, papers: 
 - AI501, Machine learning for AI, KAIST
 - AI505, Optimization for AI, KAIST
 - CS285, Deep Reinforcement Learning, UC Berkeley
    - Online course link : http://rail.eecs.berkeley.edu/deeprlcourse/
 - Stochastic Process
    - Rasmussen, Carl Edward. "Gaussian processes in machine learning." Summer school on machine learning. Springer, Berlin, Heidelberg, 2003.
    - Quinonero-Candela, Joaquin, and Carl Edward Rasmussen. "A unifying view of sparse approximate Gaussian process regression." The Journal of Machine Learning Research 6 (2005): 1939-1959.
    - Titsias, Michalis. "Variational learning of inducing variables in sparse Gaussian processes." Artificial intelligence and statistics. PMLR, 2009.
    - Gershman, Samuel J., and David M. Blei. "A tutorial on Bayesian nonparametric models." Journal of Mathematical Psychology 56.1 (2012): 1-12.
    - Teh, Yee Whye, et al. "Hierarchical dirichlet processes." Journal of the american statistical association 101.476 (2006): 1566-1581.

## Seminar

Presentation files for regular lab seminar about recent deep learning research accepted in top-tier conference of aritificial intelligence (e.g. ICML, NeurIPs, ICLR).
Papers are selected based on the my research intersets:

 - Generative Models
 - Variational Inference
 - Meta-learning
 - Reinforcement learning


## Survey

Survey on basic (and advanced) deep learning modules which many follow-up studies regard as baselines.
In macroscopic view, these can be categorized into

 - Regularization
 - Convolutional Neural Network
 - Recurrent Neural Network
 - Autoencoder
 - Meta-learning
